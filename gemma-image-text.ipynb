{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6b01b00a-a80e-4185-ba0a-e8ec0388c2bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "import json\n",
    "from PIL import Image\n",
    "from io import BytesIO\n",
    "import base64\n",
    "import subprocess\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "73bc1a1d-59c6-4df9-97e5-da4607d2f8f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------------------------\n",
    "# Step 1: Define the text prompt\n",
    "# ----------------------------------\n",
    "prompt = \"Generate a realistic image of a futuristic city skyline at sunset.\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0df7e14b-31d7-42a9-9e2f-0fb0cbf53c60",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------------------------\n",
    "# Step 2: Generate image using Ollama model\n",
    "# ----------------------------------\n",
    "# Note: Replace \"llava\" with your local image model if available\n",
    "# Gemma will be used for text prompt enhancement\n",
    "model = \"llava\"  # you can also try \"stable-diffusion\" if available\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "855b0aee-5451-4fd2-b024-da30a479fb29",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Output: I'm unable to create images directly, but I can describe the scene for you!\n",
      "\n",
      "In this image, we see a futuristic city skyline at sunset. The sky is painted with shades of orange and pink, casting a warm glow on the buildings below. Tall skyscrapers with sleek glass facades dominate the skyline, their windows reflecting the dying sunlight. Some of these buildings are adorned with colorful lights that add to the city's vibrant atmosphere.\n",
      "\n",
      "In the foreground, we see streets lined with neon signs and holographic displays, giving off a bustling vibe. The overall scene is one of advanced technology and urban living, where nature and architecture coexist harmoniously.\n"
     ]
    }
   ],
   "source": [
    "# Run Ollama command\n",
    "\n",
    "result = subprocess.run(\n",
    "    [\"ollama\", \"run\", \"llava\", prompt],\n",
    "    capture_output=True,\n",
    "    text=True,\n",
    "    encoding=\"utf-8\",   # <--- Add this line\n",
    "    errors=\"replace\"    # <--- Avoid decode errors\n",
    ")\n",
    "\n",
    "# ----------------------------------\n",
    "# Step 3: Parse and display output\n",
    "# ----------------------------------\n",
    "output = result.stdout.strip()\n",
    "\n",
    "# Some models return base64 image data\n",
    "if output.startswith(\"data:image\"):\n",
    "    img_data = output.split(\",\")[1]\n",
    "    image = Image.open(BytesIO(base64.b64decode(img_data)))\n",
    "    display(image)\n",
    "else:\n",
    "    print(\"Model Output:\", output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30c6a7d9-673a-4b9a-bacf-28bd9ecbe61c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "524a3c4c-a13a-42c1-a0cc-a9feaf74d95d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enhanced prompt: \n",
      "\n"
     ]
    }
   ],
   "source": [
    "import subprocess\n",
    "import json\n",
    "\n",
    "# Step 1: Use Gemma to improve the text prompt\n",
    "prompt = \"A cute cat sitting on a wooden table in sunlight\"\n",
    "gemma_result = subprocess.run(\n",
    "    [\"ollama\", \"run\", \"gemma:4b\", f\"Make this prompt more detailed for image generation: {prompt}\"],\n",
    "    capture_output=True, text=True, encoding=\"utf-8\"\n",
    ")\n",
    "enhanced_prompt = gemma_result.stdout.strip()\n",
    "\n",
    "print(\"Enhanced prompt:\", enhanced_prompt)\n",
    "\n",
    "# Step 2: Feed enhanced prompt into LLaVA or another image model\n",
    "image_result = subprocess.run(\n",
    "    [\"ollama\", \"run\", \"llava\", enhanced_prompt],\n",
    "    capture_output=True, text=True, encoding=\"utf-8\"\n",
    ")\n",
    "print(image_result.stdout)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e481fe05-7fbe-482e-b978-85aac1387fdf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40922e41-0fc2-4bfe-8f41-cb079b6ff71d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8543161e-816a-4090-a3d6-36fd3ecc5d3f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c37d557-91b1-4b0b-9d17-f84f069e030c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c2e63ce-c9cd-4650-a4c2-36ff0a7bcc43",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (Miniconda3)",
   "language": "python",
   "name": "miniconda3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
