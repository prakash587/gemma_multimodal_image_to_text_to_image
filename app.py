import streamlit as st
import io
import base64
from PIL import Image
import torch
import os

# ------------------------------
# Hugging Face & LLaVA
# ------------------------------
import ollama
from diffusers import AutoPipelineForText2Image

# ------------------------------
# Page Config
# ------------------------------
st.set_page_config(page_title="Multimodal App (Low VRAM)", page_icon="üß†", layout="wide")
st.title("üß† Multimodal App (Low VRAM)")
st.caption("Text ‚Üí Image (Stable Diffusion Turbo) | Image ‚Üí Text (LLaVA)")

# ------------------------------
# Load Hugging Face Token
# ------------------------------
HF_TOKEN = os.getenv("HUGGINGFACE_TOKEN", st.secrets.get("hf", {}).get("token", None))
if not HF_TOKEN:
    st.warning("‚ö†Ô∏è No Hugging Face token found. Add it to `.streamlit/secrets.toml` or as an environment variable.")
else:
    st.success("‚úÖ Hugging Face token loaded.")


# ------------------------------
# Load Stable Diffusion Turbo (Low VRAM)
# ------------------------------
@st.cache_resource(show_spinner=True)
def load_turbo():
    device = "cuda" if torch.cuda.is_available() else "cpu"

    # Use float32 on GTX 1650 (float16 can cause black images)
    dtype = torch.float32

    st.info(f"üîÑ Loading SD Turbo on {device.upper()} (low VRAM mode)‚Ä¶")

    pipe = AutoPipelineForText2Image.from_pretrained(
        "stabilityai/sd-turbo",
        torch_dtype=dtype,
        use_safetensors=True,
        cache_dir="./hf_models",
        token=HF_TOKEN
    )

    if device == "cuda":
        pipe.enable_model_cpu_offload()
        try:
            pipe.enable_xformers_memory_efficient_attention()
        except Exception:
            print("‚ö†Ô∏è xFormers not available, using default attention.")
    else:
        pipe.to("cpu")

    st.success("‚úÖ SD Turbo loaded successfully.")
    return pipe


pipe = load_turbo()

# ------------------------------
# Tabs
# ------------------------------
tab1, tab2 = st.tabs(["üé® Text ‚Üí Image", "üñºÔ∏è Image ‚Üí Text"])

# ------------------------------
# TAB 1: Text ‚Üí Image
# ------------------------------
with tab1:
    st.subheader("üé® Generate Image from Text (SD Turbo)")
    prompt = st.text_area("Enter your prompt:", "A futuristic cyberpunk city at night with neon lights", height=100)
    steps = st.slider("Inference Steps", 2, 10, 4)
    guidance = st.slider("Guidance Scale", 0.0, 2.0, 1.0)
    generate_btn = st.button("Generate Image")

    if generate_btn and prompt.strip():
        if pipe is None:
            st.error("‚ùå Model not loaded.")
        else:
            with st.spinner("Generating image‚Ä¶ ‚è≥"):
                try:
                    image = pipe(
                        prompt,
                        num_inference_steps=steps,
                        guidance_scale=guidance,
                        width=512,
                        height=512
                    ).images[0]

                    st.image(image, caption="Generated by SD Turbo", use_container_width=True)

                    buf = io.BytesIO()
                    image.save(buf, format="PNG")
                    st.download_button("üíæ Download Image", buf.getvalue(), "sd_turbo_output.png", "image/png")
                except Exception as e:
                    st.error(f"Error generating image: {e}")

# ------------------------------
# TAB 2: Image ‚Üí Text
# ------------------------------
with tab2:
    st.subheader("üñºÔ∏è Describe Image (LLaVA)")
    uploaded_image = st.file_uploader("Upload an image", type=["png", "jpg", "jpeg"])
    question = st.text_area("Ask about the image:", "Describe this image in detail.")
    analyze_btn = st.button("Analyze Image")

    if analyze_btn and uploaded_image:
        with st.spinner("Analyzing image‚Ä¶ üßê"):
            try:
                image_b64 = base64.b64encode(uploaded_image.read()).decode("utf-8")
                resp = ollama.generate(model="llava:latest", prompt=question, images=[image_b64])
                st.markdown("### üí¨ Response:")
                st.write(resp.response or "No response returned.")
            except Exception as e:
                st.error(f"Error analyzing image: {e}")

    if uploaded_image:
        st.image(uploaded_image, caption="Uploaded Image", use_container_width=True)
